{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-17T19:12:48.742375Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import convolve1d\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "## Noise Augmentation\n",
    "#Function for creating a noise vector, must be a 5000x1 array of 0's and 1's. Try to keep noise below xx\n",
    "\n",
    "def add_gaussian_noise_via_convolution_1d(data, sigma_range, E_int, max_chunk_size):\n",
    "\t\"\"\"\n",
    "\tAdd Gaussian noise through convolution using a Gaussian kernel to chunks of data with randomized sizes.\n",
    "\n",
    "\tParameters:\n",
    "\t\tdata (numpy.ndarray): Input data array (1D, e.g., 5000x1).\n",
    "\t\tsigma_range (tuple): Range of sigma values for Gaussian smoothing (min, max).\n",
    "\t\tkernel_size (int): Size of the Gaussian kernel.\n",
    "\t\tmax_chunk_size (int): The maximum chunk size for low sigma values.\n",
    "\n",
    "\tReturns:\n",
    "\t\tnumpy.ndarray: Data with added Gaussian noise.\n",
    "\t\"\"\"\n",
    "\tassert len(data.shape) == 1, \"Input data must be a 1D array.\"\n",
    "\n",
    "\tkernel_size = max(1, int(np.round(1.5 / E_int)))\n",
    "\tmodified_data = data.copy()  # Create a copy to modify\n",
    "\ttotal_length = len(data)  # Total data length\n",
    "\tidx = 0  # Start index for modification\n",
    "\n",
    "\twhile idx < total_length:\n",
    "\t\t# Randomly select a sigma value from the range\n",
    "\t\tsigma = np.random.uniform(sigma_range[0], sigma_range[1])\n",
    "\n",
    "\t\t# Scale chunk size inversely to sigma\n",
    "\t\tchunk_size = int(max_chunk_size * (1 - (sigma / sigma_range[1])))\n",
    "\t\tchunk_size = max(1, chunk_size)  # Ensure at least one element is modified\n",
    "\n",
    "\t\t# Determine end index for this chunk\n",
    "\t\tend_idx = min(idx + chunk_size, total_length)\n",
    "\n",
    "\t\t# Extract the current chunk\n",
    "\t\tchunk = modified_data[idx:end_idx]\n",
    "\n",
    "\t\t# Generate a 1D Gaussian kernel\n",
    "\t\tax = np.linspace(-(kernel_size - 1) / 2., (kernel_size - 1) / 2., kernel_size)\n",
    "\t\tkernel = np.exp(-0.5 * (ax / sigma) ** 2)\n",
    "\t\tkernel /= np.sum(kernel)  # Normalize the kernel\n",
    "\n",
    "\t\t# Convolve the kernel with the 1D chunk\n",
    "\t\tsmoothed_chunk = convolve1d(chunk, kernel, mode='reflect')\n",
    "\n",
    "\t\t# Add random Gaussian noise\n",
    "\t\tnoise = np.random.normal(loc=0, scale=sigma, size=chunk.shape)\n",
    "\t\tnoisy_chunk = smoothed_chunk + noise\n",
    "\n",
    "\t\t# Update the modified data with the noisy chunk\n",
    "\t\tmodified_data[idx:end_idx] = noisy_chunk\n",
    "\n",
    "\t\t# Move to the next chunk\n",
    "\t\tidx = end_idx\n",
    "\n",
    "\treturn modified_data\n",
    "\n",
    "def random_broadening(x: np.array, y: np.array, random_seed=42, sigma_min=10, sigma_max=100, kernel_size=16):\n",
    "\tbroadened_y = np.zeros_like(y)\n",
    "\thalf_kernel_size = kernel_size // 2\n",
    "\tx_kernel = np.linspace(-5, 5, kernel_size)\n",
    "\n",
    "\tnp.random.seed(random_seed)  # random seed for reproducibility\n",
    "\trandom_sigmas = np.random.uniform(sigma_min, sigma_max, size=len(x))  # random sigmas for each point\n",
    "\n",
    "\tfor i in range(len(x)):\n",
    "\t\t# initialize kernel\n",
    "\t\tsigma = random_sigmas[i]\n",
    "\t\tkernel = np.exp(-x_kernel ** 2 / (2 * sigma ** 2))\n",
    "\t\tkernel /= kernel.sum()  # normalize kernel\n",
    "\n",
    "\t\tstart = max(0, i - half_kernel_size)\n",
    "\t\tend = min(len(x), i + half_kernel_size + 1)\n",
    "\n",
    "\t\tkernel_start = max(0, half_kernel_size - i)\n",
    "\t\tkernel_end = kernel_start + (end - start)\n",
    "\n",
    "\t\tif kernel_end > len(kernel):\n",
    "\t\t\tkernel_end = len(kernel)\n",
    "\t\t\tend = start + (kernel_end - kernel_start)\n",
    "\n",
    "\t\tbroadened_y[start:end] += y[i] * kernel[kernel_start:kernel_end]\n",
    "\n",
    "\treturn broadened_y\n",
    "\n",
    "## Shift Augmentation\n",
    "def shift_spectra(energy_interval, energy_shift, intensity):\n",
    "\trandom_shift = np.random.uniform(-energy_shift, energy_shift, size=(intensity.shape[0], 1))\n",
    "\tshifted_energy = random_shift / energy_interval\n",
    "\tshifted_energy = shifted_energy.astype(int)\n",
    "\tintensity = intensity.copy()\n",
    "\tfor idx, inten in enumerate(intensity):\n",
    "\t\tintensity[idx] = np.roll(inten, shifted_energy[idx])\n",
    "\n",
    "\treturn intensity\n",
    "\n",
    "\n",
    "## Total Augmentation pipeline\n",
    "\n",
    "#load dataset\n",
    "path = '../dataset'\n",
    "x_train = np.load(f'{path}/x_train_use.npy')\n",
    "y_train = np.load(f'{path}/y_train_use.npy')\n",
    "\n",
    "\n",
    "# Add noise to the training data\n",
    "\n",
    "def add_noise_total(x_train, energy_interval, y_train):\n",
    "\taugment_strength = [0.01, 0.05, 0.1]\n",
    "\tx_train_augment = x_train.copy()\n",
    "\tx_train_augmented = np.zeros((len(augment_strength), x_train.shape[0], x_train.shape[1]))\n",
    "\tfor idx1, x_train_sample in enumerate(x_train_augment):\n",
    "\t\tfor idx, i in enumerate(augment_strength):\n",
    "\t\t\tsigma_min_max = (i * np.max(x_train_sample), (i + 0.05) * np.max(x_train_sample))\n",
    "\t\t\tx_train_augmented[idx][idx1] = add_gaussian_noise_via_convolution_1d(x_train_sample, sigma_min_max, 0.3, 300)\n",
    "\n",
    "\ty_train_augmented = y_train.copy()\n",
    "\ty_train_augmented = y_train_augmented.reshape((1, y_train_augmented.shape[0], y_train_augmented.shape[1]))\n",
    "\ty_train_augmented = np.tile(y_train_augmented, (x_train_augmented.shape[0], 1, 1))\n",
    "\treturn x_train_augmented, y_train_augmented\n",
    "\n",
    "\n",
    "def add_shift_total(x_train, energy_interval, y_train):\n",
    "\tenergy_shift = 4\n",
    "\tx_train_augment = x_train.copy()\n",
    "\tx_train_augmented = np.zeros((1, x_train.shape[0], x_train.shape[1]))\n",
    "\tfor idx1, x_train_sample in enumerate(x_train_augment):\n",
    "\t\tx_train_augmented[0][idx1] = shift_spectra(0.3, energy_shift, x_train_sample)\n",
    "\n",
    "\ty_train_augmented = y_train.copy()\n",
    "\ty_train_augmented = y_train_augmented.reshape((1, y_train_augmented.shape[0], y_train_augmented.shape[1]))\n",
    "\ty_train_augmented = np.tile(y_train_augmented, (x_train_augmented.shape[0], 1, 1))\n",
    "\treturn x_train_augmented, y_train_augmented\n",
    "\n",
    "\n",
    "def add_broadening_total(x_train, y_train):\n",
    "\tx_train_augment = x_train.copy()\n",
    "\taugment_strength = [10, 20, 30]\n",
    "\tx_train_augmented = np.zeros((len(augment_strength), x_train.shape[0], x_train.shape[1]))\n",
    "\tfor idx1, x_train_sample in enumerate(x_train_augment):\n",
    "\t\tfor idx, i in enumerate(augment_strength):\n",
    "\t\t\tx = np.arange(len(x_train_sample))\n",
    "\t\t\tx_train_augmented[idx][idx1] = random_broadening(x, x_train_sample, random_seed=42, sigma_min=10, sigma_max=100,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t kernel_size=i)\n",
    "\n",
    "\ty_train_augmented = y_train.copy()\n",
    "\ty_train_augmented = y_train_augmented.reshape((1, y_train_augmented.shape[0], y_train_augmented.shape[1]))\n",
    "\ty_train_augmented = np.tile(y_train_augmented, (x_train_augmented.shape[0], 1, 1))\n",
    "\treturn x_train_augmented, y_train_augmented\n",
    "\n",
    "\n",
    "def add_augment_total(x_train, energy_interval, y_train):\n",
    "\tnoise_augment = add_noise_total(x_train, energy_interval, y_train)\n",
    "\tshift_augment = add_shift_total(x_train, energy_interval, y_train)\n",
    "\tbroaden_augment = add_broadening_total(x_train, y_train)\n",
    "\ttotal_augment_x = np.concatenate((noise_augment[0], shift_augment[0], broaden_augment[0]), axis=0)\n",
    "\ttotal_augment_y = np.concatenate((noise_augment[1], shift_augment[1], broaden_augment[1]), axis=0)\n",
    "\treturn total_augment_x, total_augment_y\n",
    "\n",
    "\n",
    "energy_interval = 'use'\n",
    "x_train_augmented, y_train_augmented = add_augment_total(x_train, energy_interval, y_train)\n",
    "x_train_augmented_res = x_train_augmented.reshape(\n",
    "\t(x_train_augmented.shape[0] * x_train_augmented.shape[1], x_train_augmented.shape[2]))\n",
    "y_train_augmented_res = y_train_augmented.reshape(\n",
    "\t(y_train_augmented.shape[0] * y_train_augmented.shape[1], y_train_augmented.shape[2]))\n",
    "x_train_total = np.concatenate((x_train, x_train_augmented_res), axis=0)\n",
    "y_train_total = np.concatenate((y_train, y_train_augmented_res), axis=0)\n",
    "np.save(f'{path}/x_train_augmented_{energy_interval}.npy', x_train_total)\n",
    "np.save(f'{path}/y_train_augmented_{energy_interval}.npy', y_train_total)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
