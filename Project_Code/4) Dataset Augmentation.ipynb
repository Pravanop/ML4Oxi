{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T00:02:15.883679Z",
     "start_time": "2024-12-17T00:02:15.871123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import convolve1d\n",
    "from matplotlib import pyplot as plt"
   ],
   "id": "cb94e38c23edc88",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Noise Augmentation",
   "id": "94496198d070e9b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:07:07.777246Z",
     "start_time": "2024-12-17T01:07:07.771237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Function for creating a noise vector, must be a 5000x1 array of 0's and 1's. Try to keep noise below xx\n",
    "\n",
    "def add_gaussian_noise_via_convolution_1d(data, sigma_range, E_int, max_chunk_size):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise through convolution using a Gaussian kernel to chunks of data with randomized sizes.\n",
    "\n",
    "    Parameters:\n",
    "        data (numpy.ndarray): Input data array (1D, e.g., 5000x1).\n",
    "        sigma_range (tuple): Range of sigma values for Gaussian smoothing (min, max).\n",
    "        E_int (float): Energy interval of the device (eV).\n",
    "        max_chunk_size (int): The maximum chunk size for low sigma values.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Data with added Gaussian noise.\n",
    "    \"\"\"\n",
    "    assert len(data.shape) == 1, \"Input data must be a 1D array.\"\n",
    "\n",
    "    kernel_size = max(1, int(np.round(1.5 / E_int))) \n",
    "    modified_data = data.copy()  # Create a copy to modify\n",
    "    total_length = len(data)  # Total data length\n",
    "    idx = 0  # Start index for modification\n",
    "\n",
    "    while idx < total_length:\n",
    "        # Randomly select a sigma value from the range\n",
    "        sigma = np.random.uniform(sigma_range[0], sigma_range[1])\n",
    "        \n",
    "        # Scale chunk size inversely to sigma\n",
    "        chunk_size = int(max_chunk_size * (1 - (sigma / sigma_range[1])))\n",
    "        chunk_size = max(1, chunk_size)  # Ensure at least one element is modified\n",
    "        \n",
    "        # Determine end index for this chunk\n",
    "        end_idx = min(idx + chunk_size, total_length)\n",
    "        \n",
    "        # Extract the current chunk\n",
    "        chunk = modified_data[idx:end_idx]\n",
    "        \n",
    "        # Generate a 1D Gaussian kernel\n",
    "        ax = np.linspace(-(kernel_size - 1) / 2., (kernel_size - 1) / 2., kernel_size)\n",
    "        kernel = np.exp(-0.5 * (ax / sigma) ** 2)\n",
    "        kernel /= np.sum(kernel) + 1e-6  # Normalize the kernel\n",
    "        \n",
    "        # Convolve the kernel with the 1D chunk\n",
    "        smoothed_chunk = convolve1d(chunk, kernel, mode='reflect')\n",
    "        \n",
    "        # Add random Gaussian noise\n",
    "        noise = np.random.normal(loc=0, scale=sigma, size=chunk.shape)\n",
    "        noisy_chunk = smoothed_chunk + noise\n",
    "        \n",
    "        # Update the modified data with the noisy chunk\n",
    "        modified_data[idx:end_idx] = noisy_chunk\n",
    "        \n",
    "        # Move to the next chunk\n",
    "        idx = end_idx\n",
    "\n",
    "    return modified_data\n"
   ],
   "id": "2cff00bbe21de9d8",
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:07:08.000921Z",
     "start_time": "2024-12-17T01:07:07.998822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example Usage:\n",
    "# Create a sample 1D array (5000x1 vector)\n",
    "# data = np.linspace(0,100,5000)\n",
    "# data = np.sin(data)\n",
    "# # Apply Gaussian noise via convolution\n",
    "# noisy_data = add_gaussian_noise_via_convolution_1d(data, (0.01,0.3), 0.3, 300)\n",
    "#\n",
    "# # Plot the results\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(data, label=\"Original Data\")\n",
    "# plt.plot(noisy_data, label=\"Noisy Data\", alpha=0.8)\n",
    "# plt.legend()\n",
    "# plt.title(\"Adding Gaussian Noise to 1D Data via Convolution\")\n",
    "# plt.show()"
   ],
   "id": "7b305b2263ea90a",
   "outputs": [],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-12-17T01:07:08.145435Z",
     "start_time": "2024-12-17T01:07:08.142130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_randomized_noise(data, noise_range, max_chunk_size):\n",
    "    \"\"\"\n",
    "    Add randomized Gaussian noise to chunks of data.\n",
    "\n",
    "    Parameters:\n",
    "        data (numpy.ndarray): Input data array (1D, e.g., 5000x1).\n",
    "        noise_range (tuple): Range for random noise multiplier (min, max).\n",
    "        max_chunk_size (int): The maximum chunk size for low noise values.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Data with added noise.\n",
    "    \"\"\"\n",
    "    assert len(data.shape) == 1, \"Input data must be a 1D array.\"\n",
    "    \n",
    "    modified_data = data.copy()  # Create a copy to modify\n",
    "    total_length = len(data)  # Total data length\n",
    "    idx = 0  # Start index for modification\n",
    "\n",
    "    while idx < total_length:\n",
    "        # Randomly select a noise multiplier from the range\n",
    "        noise_multiplier = np.random.uniform(noise_range[0], noise_range[1])\n",
    "        \n",
    "        # Scale chunk size inversely to noise magnitude\n",
    "        chunk_size = int(max_chunk_size * (1 - noise_multiplier))\n",
    "        chunk_size = max(1, chunk_size)  # Ensure at least one element is modified\n",
    "        \n",
    "        # Determine end index for this chunk\n",
    "        end_idx = min(idx + chunk_size, total_length)\n",
    "        \n",
    "        # Apply noise to the current chunk\n",
    "        modified_data[idx:end_idx] *= (1 + noise_multiplier)\n",
    "        \n",
    "        # Move to the next chunk\n",
    "        idx = end_idx\n",
    "\n",
    "    return modified_data"
   ],
   "id": "e620b5812071f107",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:07:08.305467Z",
     "start_time": "2024-12-17T01:07:08.303536Z"
=======
     "end_time": "2024-12-17T00:02:17.409838Z",
     "start_time": "2024-12-17T00:02:17.407841Z"
>>>>>>> origin/main
    }
   },
   "cell_type": "code",
   "source": [
    "# Example Usage:\n",
    "# Create a sample 1D array (5000x1 vector)\n",
    "# data = np.random.random(5000)\n",
    "# noise_range = (0, 0.5)  # Noise range\n",
    "# modified_data = add_randomized_noise(data, noise_range,100)\n",
    "#\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(data, label=\"Original Data\")\n",
    "# plt.plot(noisy_data, label=\"Noisy Data\", alpha=0.8)\n",
    "# plt.legend()\n",
    "# plt.title(\"Adding Gaussian Noise to 1D Data via Convolution\")\n",
    "# plt.show()"
   ],
   "id": "977659f28da81554",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Instrumental Broadening",
   "id": "285861a68b0c4bbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:07:08.641161Z",
     "start_time": "2024-12-17T01:07:08.636281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def random_broadening(x: np.array, y: np.array, random_seed=42, sigma_min=10, sigma_max=100, kernel_size=16):\n",
    "    broadened_y = np.zeros_like(y)\n",
    "    half_kernel_size = kernel_size // 2\n",
    "    x_kernel = np.linspace(-5, 5, kernel_size)\n",
    "\n",
    "    np.random.seed(random_seed)  # random seed for reproducibility\n",
    "    random_sigmas = np.random.uniform(sigma_min, sigma_max, size=len(x))  # random sigmas for each point\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        # initialize kernel\n",
    "        sigma = random_sigmas[i]\n",
    "        kernel = np.exp(-x_kernel**2 / (2 * sigma**2))\n",
    "        kernel /= kernel.sum()  # normalize kernel\n",
    "\n",
    "        start = max(0, i - half_kernel_size)\n",
    "        end = min(len(x), i + half_kernel_size + 1)\n",
    "\n",
    "\n",
    "        kernel_start = max(0, half_kernel_size - i)\n",
    "        kernel_end = kernel_start + (end - start)\n",
    "\n",
    "        if kernel_end > len(kernel): \n",
    "            kernel_end = len(kernel)\n",
    "            end = start + (kernel_end - kernel_start)\n",
    "\n",
    "        broadened_y[start:end] += y[i] * kernel[kernel_start:kernel_end]\n",
    "\n",
    "    return broadened_y"
   ],
   "id": "d0db7e9f3c4477d6",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:07:08.818406Z",
     "start_time": "2024-12-17T01:07:08.816680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example Usage:\n",
    "# y = np.load('../dataset/x_test.npy')[59]\n",
    "# x = np.arange(len(y))\n",
    "#\n",
    "# y_broadened = random_broadening(x, y, random_seed=42, sigma_min=10, sigma_max=100, kernel_size=16)\n",
    "#\n",
    "#\n",
    "#\n",
    "# plt.plot(x, y, label=\"Original Data\")\n",
    "# plt.plot(x, y_broadened, label=\"Broadened Data\")\n",
    "# plt.xlim(4300, 5000)\n"
   ],
   "id": "acc5672b0761fe8d",
   "outputs": [],
   "execution_count": 155
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Shift Augmentation",
   "id": "52ca3d476108d2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:07:09.151624Z",
     "start_time": "2024-12-17T01:07:09.148488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def shift_spectra(energy_interval, energy_shift, intensity):\n",
    "\n",
    "\trandom_shift = np.random.uniform(-energy_shift, energy_shift, size = (intensity.shape[0], 1))\n",
    "\tshifted_energy = random_shift/energy_interval\n",
    "\tshifted_energy = shifted_energy.astype(int)\n",
    "\tintensity = intensity.copy()\n",
    "\tfor idx, inten in enumerate(intensity):\n",
    "\t\tintensity[idx] = np.roll(inten, shifted_energy[idx])\n",
    "\n",
    "\treturn intensity\n"
   ],
   "id": "67a7f78382da2204",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Total Augmentation pipeline",
   "id": "d0c77c319313dfe8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:12:06.892509Z",
     "start_time": "2024-12-17T01:12:06.870359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "#load dataset\n",
    "path = '../dataset'\n",
    "energy_interval = 0.5\n",
    "x_train = np.load(f'{path}/x_train_{energy_interval}.npy')\n",
    "y_train = np.load(f'{path}/y_train_{energy_interval}.npy')"
   ],
   "id": "8112d928d31c8825",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:12:07.033279Z",
     "start_time": "2024-12-17T01:12:07.031206Z"
    }
   },
   "cell_type": "code",
   "source": "print(x_train.shape, y_train.shape)",
   "id": "1782078bf1002e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4678, 1040) (4678, 8)\n"
     ]
    }
   ],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:12:07.176268Z",
     "start_time": "2024-12-17T01:12:07.172842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add noise to the training data\n",
    "\n",
    "def add_noise_total(x_train, energy_interval, y_train):\n",
    "\n",
    "    augment_strength = [0.01, 0.05, 0.1]\n",
    "    x_train_augment = x_train.copy()\n",
    "    x_train_augmented = np.zeros((len(augment_strength), x_train.shape[0], x_train.shape[1]))\n",
    "    for idx1, x_train_sample in enumerate(x_train_augment):\n",
    "        for idx, i in enumerate(augment_strength):\n",
    "            sigma_min_max = (i*np.max(x_train_sample), (i+0.05)*np.max(x_train_sample))\n",
    "            x_train_augmented[idx][idx1] = add_gaussian_noise_via_convolution_1d(x_train_sample, sigma_min_max, energy_interval, 300)\n",
    "\n",
    "    y_train_augmented = y_train.copy()\n",
    "    y_train_augmented = y_train_augmented.reshape((1, y_train_augmented.shape[0], y_train_augmented.shape[1]))\n",
    "    y_train_augmented = np.tile(y_train_augmented, (x_train_augmented.shape[0], 1, 1))\n",
    "    return x_train_augmented, y_train_augmented\n"
   ],
   "id": "c6b9b0dbeb76082a",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:12:07.315522Z",
     "start_time": "2024-12-17T01:12:07.312517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_shift_total(x_train, energy_interval, y_train):\n",
    "    energy_shift = 4\n",
    "    x_train_augment = x_train.copy()\n",
    "    x_train_augmented = np.zeros((1, x_train.shape[0], x_train.shape[1]))\n",
    "    for idx1, x_train_sample in enumerate(x_train_augment):\n",
    "        x_train_augmented[0][idx1] = shift_spectra(energy_interval, energy_shift, x_train_sample)\n",
    "\n",
    "    y_train_augmented = y_train.copy()\n",
    "    y_train_augmented = y_train_augmented.reshape((1, y_train_augmented.shape[0], y_train_augmented.shape[1]))\n",
    "    y_train_augmented = np.tile(y_train_augmented, (x_train_augmented.shape[0], 1, 1))\n",
    "    return x_train_augmented, y_train_augmented"
   ],
   "id": "623a5f8400a5969b",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:12:07.459088Z",
     "start_time": "2024-12-17T01:12:07.456014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_broadening_total(x_train, y_train):\n",
    "    x_train_augment = x_train.copy()\n",
    "    augment_strength = [10, 20, 30]\n",
    "    x_train_augmented = np.zeros((len(augment_strength), x_train.shape[0], x_train.shape[1]))\n",
    "    for idx1, x_train_sample in enumerate(x_train_augment):\n",
    "        for idx, i in enumerate(augment_strength):\n",
    "            x = np.arange(len(x_train_sample))\n",
    "            x_train_augmented[idx][idx1] = random_broadening(x, x_train_sample, random_seed=42, sigma_min=10, sigma_max=100, kernel_size=i)\n",
    "\n",
    "    y_train_augmented = y_train.copy()\n",
    "    y_train_augmented = y_train_augmented.reshape((1, y_train_augmented.shape[0], y_train_augmented.shape[1]))\n",
    "    y_train_augmented = np.tile(y_train_augmented, (x_train_augmented.shape[0], 1, 1))\n",
    "    return x_train_augmented, y_train_augmented"
   ],
   "id": "53bc418c91ab35f5",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:12:07.682422Z",
     "start_time": "2024-12-17T01:12:07.679662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_augment_total(x_train, energy_interval, y_train):\n",
    "    noise_augment = add_noise_total(x_train, energy_interval, y_train)\n",
    "    shift_augment = add_shift_total(x_train, energy_interval, y_train)\n",
    "    broaden_augment = add_broadening_total(x_train, y_train)\n",
    "    total_augment_x = np.concatenate((noise_augment[0], shift_augment[0], broaden_augment[0]), axis = 0)\n",
    "    total_augment_y = np.concatenate((noise_augment[1], shift_augment[1], broaden_augment[1]), axis = 0)\n",
    "    return total_augment_x, total_augment_y"
   ],
   "id": "4a52b8247f536aa6",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:13:49.920280Z",
     "start_time": "2024-12-17T01:12:07.824021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train_augmented, y_train_augmented = add_augment_total(x_train, energy_interval, y_train)\n",
    "x_train_augmented.shape"
   ],
   "id": "dbda17c8815661d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4678, 1040)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:13:50.005242Z",
     "start_time": "2024-12-17T01:13:49.997726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train_augmented_res = x_train_augmented.reshape((x_train_augmented.shape[0]*x_train_augmented.shape[1], x_train_augmented.shape[2]))\n",
    "y_train_augmented_res = y_train_augmented.reshape((y_train_augmented.shape[0]*y_train_augmented.shape[1], y_train_augmented.shape[2]))"
   ],
   "id": "146b7ce328a3d7b5",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:13:50.082690Z",
     "start_time": "2024-12-17T01:13:50.021245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train_total = np.concatenate((x_train, x_train_augmented_res), axis = 0)\n",
    "y_train_total = np.concatenate((y_train, y_train_augmented_res), axis = 0)"
   ],
   "id": "a5f2d4abaf9db133",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T01:13:50.222646Z",
     "start_time": "2024-12-17T01:13:50.085833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.save(f'{path}/x_train_augmented_{energy_interval}.npy', x_train_total)\n",
    "np.save(f'{path}/y_train_augmented_{energy_interval}.npy', y_train_total)"
   ],
   "id": "169325dd895ea591",
   "outputs": [],
   "execution_count": 176
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "14bfbf52bc2efa6d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
